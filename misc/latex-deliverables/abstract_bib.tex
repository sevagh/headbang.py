\documentclass[letter,12pt]{report}
%\setlength{\parindent}{0pt}
\usepackage[left=2cm, right=2cm, top=2cm, bottom=2cm]{geometry}
\usepackage[shortlabels]{enumitem}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{verbatim}
\usepackage{listings}
\usepackage{minted}
\usepackage{subfig}
\usepackage{titling}
\usepackage{caption}
\setlength{\droptitle}{1cm}
\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=black,
    urlcolor=black
}
\usepackage{setspace}
\renewcommand{\topfraction}{0.85}
\renewcommand{\textfraction}{0.1}
\renewcommand{\floatpagefraction}{0.75}
\usepackage[backend=biber,authordate]{biblatex-chicago}
\addbibresource{citations.bib}
\usepackage{titlesec}
 
\titleformat{\chapter}[display]
  {\normalfont\bfseries}{}{0pt}{\Huge}

\begin{document}

\noindent\Large{\textbf{headbang.py}}\\
\large{Final project abstract. MUMT 621, April 13, 2021}\\
\large{Sevag Hanssian, 260398537}

\noindent\hrulefill

\vspace{2em}

headbang.py is a Python library and collection of command-line tools which explored two aspects of MIR beat tracking:

\begin{enumerate}
	\item
		Audio beat tracking which considered the outputs of 6 separate beat tracking algorithms in a consensus, and aligned the result with strong percussive onsets in progressive metal music. The goal was to locate strong beats where a human may headbang due to the psychological phenomenon of groove (\cite{groove}). Owing to the lack of ground truth beat annotations, the testing was performed manually and several compelling examples were shown in the demo website. Future ideas include using the beat tracking outputs to drive some form of beat-related visualization, such as an automated flashing light show or automatic music video generation.\\

		\vspace{-0.5em}
		In addition to the consensus beat tracker, a visual animation tool was created to compare the outputs of the different beat trackers. This tool could be be used to further improve the consensus beat tracking algorithm through visual observation, which could help create a bespoke consensus algorithm that works best for a particular genre of music in different projects.
	\item
		Headbanging motion analysis of human subjects in metal musical videos (e.g., concert footage, or clips of musicians playing instruments). The OpenPose 2D pose estimation library (\cite{openpose}) was used to analyze peaks in the vertical motion of the head and face, to track headbanging motion and tempo alongside audio beat and tempo tracking analysis. Two experimental ideas were presented as additional arguments to the script, including supplementing audio beat tracking with motion-detected beat annotations, and detection of very groovy sections of songs where multiple subjects in the video were headbanging in unison (indicating a strong agreement on the beat).
\end{enumerate}

The source code of headbang.py is available,\footnote{\url{https://github.com/sevagh/headbang.py}} and there is also a website\footnote{\url{https://sevagh.github.io/headbang.py}} with design documents, diagrams, and audio and video sample outputs.

\vfill
\clearpage

\nocite{*}
\printbibheading[title={\vspace{-3.5em}References},heading=bibnumbered]
\vspace{-1.5em}
\printbibliography[heading=none]

\end{document}
